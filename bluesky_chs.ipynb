{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6df6d9",
   "metadata": {},
   "source": [
    "# Use JSON file from real-time data on the GHS pipeline \n",
    "\n",
    "This notebook runs the `src/analyze_api.py` workflow inline so you can inspect intermediate results and tweak parameters interactively.\n",
    "\n",
    "Sections:\n",
    "\n",
    "1. Setup imports & paths\n",
    "2. Load pipeline helpers from `src`\n",
    "3. Compute overall per-city CHS and per-post time series (with progress bars)\n",
    "4. Save results to `results/api_1_analysis.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85252c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup imports and paths\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# progress bar (optional)\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **_kw):\n",
    "        return x\n",
    "\n",
    "ROOT = Path('.')  # adjust if running from another cwd\n",
    "DATA_PATH = ROOT / 'api_1.json'\n",
    "OUT_DIR = ROOT / 'results'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "OUT_FILE = OUT_DIR / 'api_1_analysis.json'\n",
    "\n",
    "def ts_to_iso(ts):\n",
    "    try:\n",
    "        return datetime.fromtimestamp(int(ts)).isoformat() + 'Z'\n",
    "    except Exception:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58fdd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianyang/micromamba/envs/HTB2026/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded: classify_with_sentiment, compute_topic_signed_scores, normalize_topic_scores_0_10, compute_CHS\n"
     ]
    }
   ],
   "source": [
    "# 2) Import the modularised pipeline helpers from src\n",
    "# These are the functions you moved into src/ earlier.\n",
    "try:\n",
    "    from src.classify_sentiment import classify_with_sentiment\n",
    "    from src.CHS_computation import (\n",
    "        compute_topic_signed_scores,\n",
    "        normalize_topic_scores_0_10,\n",
    "        compute_CHS,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError('Could not import pipeline modules. Ensure your PYTHONPATH and dependencies (transformers) are installed.') from e\n",
    "\n",
    "print('Modules loaded: classify_with_sentiment, compute_topic_signed_scores, normalize_topic_scores_0_10, compute_CHS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a1ed6",
   "metadata": {},
   "source": [
    "## 3) Run analysis per city (overall + per-post time series)\n",
    "Runs the same steps as `src/analyze_api.py` but keeps results in the notebook for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d311faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cities:   0%|          | 0/186 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "/tmp/ipykernel_3398638/2808195562.py:21: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(int(ts)).isoformat() + 'Z'\n",
      "Cities: 100%|██████████| 186/186 [13:24<00:00,  4.33s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote analysis to: results/api_1_analysis.json\n",
      "Reading n_posts= 23 overall_chs= 5.0\n",
      "Worth n_posts= 14 overall_chs= 4.7\n",
      "Liverpool n_posts= 13 overall_chs= 4.8\n",
      "Deal n_posts= 10 overall_chs= 4.7\n",
      "Boston n_posts= 8 overall_chs= 5.0\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "for city in tqdm(data, desc='Cities'):\n",
    "    city_name = city.get('city')\n",
    "    posts = city.get('posts', []) or []\n",
    "\n",
    "    # overall analysis using all texts\n",
    "    texts_all = [p.get('text', '') for p in posts if p.get('text')]\n",
    "    if texts_all:\n",
    "        raw, details = compute_topic_signed_scores(texts_all, classify_with_sentiment, topk_mean=3, threshold=0.25)\n",
    "        topic_0_10 = normalize_topic_scores_0_10(raw)\n",
    "        chs = compute_CHS(topic_0_10)\n",
    "    else:\n",
    "        topic_0_10 = {t: 0.0 for t in []}\n",
    "        chs = 0.0\n",
    "\n",
    "    # per-post time series (chronological)\n",
    "    timeseries = []\n",
    "    posts_sorted = sorted(posts, key=lambda p: p.get('posted_at_timestamp', 0))\n",
    "    for p in tqdm(posts_sorted, desc=f'Posts ({city_name})', leave=False):\n",
    "        ts = p.get('posted_at_timestamp')\n",
    "        text = p.get('text', '')\n",
    "        if not text:\n",
    "            continue\n",
    "        raw_post, details_post = compute_topic_signed_scores([text], classify_with_sentiment, topk_mean=3, threshold=0.25)\n",
    "        topic_post_0_10 = normalize_topic_scores_0_10(raw_post)\n",
    "        chs_post = compute_CHS(topic_post_0_10)\n",
    "\n",
    "        timeseries.append({\n",
    "            'posted_at_timestamp': ts,\n",
    "            'iso': ts_to_iso(ts) if ts is not None else '',\n",
    "            'topic_scores_0_10': topic_post_0_10,\n",
    "            'chs': chs_post,\n",
    "            'text': text,\n",
    "        })\n",
    "\n",
    "    results.append({\n",
    "        'city': city_name,\n",
    "        'lat': city.get('lat'),\n",
    "        'lng': city.get('lng'),\n",
    "        'n_posts': len(posts),\n",
    "        'overall_topic_scores_0_10': topic_0_10,\n",
    "        'overall_chs': chs,\n",
    "        'timeseries': timeseries,\n",
    "    })\n",
    "\n",
    "# save results\n",
    "with open(OUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'Wrote analysis to: {OUT_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac764dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading n_posts= 23 overall_topic_scores_0_10= {'education': 5.0, 'life_satisfaction': 8.4, 'income': 4.9, 'access_to_services': 5.0, 'housing': 7.4, 'community': 3.5, 'jobs': 2.3, 'civic_engagement': 5.7, 'environment': 5.0, 'safety': 5.0, 'health': 5.0} overall_chs= 5.0\n",
      "Worth n_posts= 14 overall_topic_scores_0_10= {'jobs': 5.0, 'income': 6.3, 'community': 2.7, 'civic_engagement': 6.4, 'life_satisfaction': 5.0, 'access_to_services': 5.0, 'safety': 2.1, 'environment': 5.0, 'health': 5.0, 'housing': 5.0, 'education': 5.0} overall_chs= 4.7\n",
      "Liverpool n_posts= 13 overall_topic_scores_0_10= {'community': 4.2, 'life_satisfaction': 8.8, 'civic_engagement': 5.0, 'access_to_services': 5.4, 'safety': 5.0, 'jobs': 1.5, 'income': 5.0, 'environment': 5.0, 'health': 5.0, 'housing': 5.0, 'education': 5.0} overall_chs= 4.8\n",
      "Deal n_posts= 10 overall_topic_scores_0_10= {'life_satisfaction': 5.8, 'access_to_services': 4.2, 'civic_engagement': 5.0, 'income': 5.0, 'housing': 1.3, 'environment': 5.0, 'safety': 5.0, 'health': 5.0, 'education': 5.0, 'community': 5.0, 'jobs': 5.0} overall_chs= 4.7\n",
      "Boston n_posts= 8 overall_topic_scores_0_10= {'safety': 5.0, 'life_satisfaction': 5.0, 'jobs': 5.0, 'income': 5.0, 'civic_engagement': 5.0, 'education': 5.5, 'access_to_services': 5.0, 'environment': 5.0, 'health': 5.0, 'housing': 5.0, 'community': 5.0} overall_chs= 5.0\n"
     ]
    }
   ],
   "source": [
    "# Show a short summary\n",
    "for r in results[:5]:\n",
    "    print(r['city'], 'n_posts=', r['n_posts'], 'overall_topic_scores_0_10=', r['overall_topic_scores_0_10'], 'overall_chs=', round(r['overall_chs'],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2efee0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch helpers: use with your existing src.* modules\n",
    "from math import ceil\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import src.classify_sentiment as cs  # uses zshot, sentiment_clf, OECD_CATEGORIES\n",
    "# optional: import compute helpers if you want to reuse them\n",
    "from src.CHS_computation import CHS_WEIGHTS, OECD_TOPICS\n",
    "\n",
    "def batch_classify_texts(texts, topk_mean=None, threshold=0.25, batch_size=32):\n",
    "    \"\"\"\n",
    "    Return list of dicts equivalent to classify_with_sentiment(text).\n",
    "    - texts: list[str]\n",
    "    - topk_mean: passed to label aggregation\n",
    "    - batch_size: model batch size for pipeline\n",
    "    \"\"\"\n",
    "    T = len(texts)\n",
    "    # per-category arrays: cat_scores[cat][i] = aggregated score for texts[i] for this category\n",
    "    cat_scores = {cat: [0.0] * T for cat in cs.OECD_CATEGORIES.keys()}\n",
    "    cat_best_label = {cat: [None] * T for cat in cs.OECD_CATEGORIES.keys()}\n",
    "    cat_label_scores = {cat: [None] * T for cat in cs.OECD_CATEGORIES.keys()}\n",
    "\n",
    "    # 1) For each category, run zshot on texts in chunks\n",
    "    for cat, labels in cs.OECD_CATEGORIES.items():\n",
    "        for start in range(0, T, batch_size):\n",
    "            batch_texts = texts[start:start+batch_size]\n",
    "            # pipeline accepts a list of sequences and returns list-of-dicts\n",
    "            results = cs.zshot(batch_texts, candidate_labels=labels, multi_label=True, batch_size=batch_size)\n",
    "            for idx, out in enumerate(results, start=start):\n",
    "                label_scores = list(zip(out[\"labels\"], out[\"scores\"]))\n",
    "                label_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                if topk_mean and topk_mean > 1:\n",
    "                    k = min(topk_mean, len(label_scores))\n",
    "                    agg = float(np.mean([s for _, s in label_scores[:k]]))\n",
    "                else:\n",
    "                    agg = float(label_scores[0][1])\n",
    "                cat_scores[cat][idx] = agg\n",
    "                cat_best_label[cat][idx] = label_scores[0][0]\n",
    "                cat_label_scores[cat][idx] = label_scores\n",
    "\n",
    "    # 2) Sentiment for all texts in batches\n",
    "    sent_results = []\n",
    "    for start in range(0, T, batch_size):\n",
    "        batch_texts = texts[start:start+batch_size]\n",
    "        sent_batch = cs.sentiment_clf(batch_texts, batch_size=batch_size)\n",
    "        sent_results.extend(sent_batch)\n",
    "\n",
    "    # 3) Build per-text outputs (same keys as classify_with_sentiment)\n",
    "    outputs = []\n",
    "    for i in range(T):\n",
    "        per_cat = {cat: cat_scores[cat][i] for cat in cs.OECD_CATEGORIES.keys()}\n",
    "        best_cat, best_score = max(per_cat.items(), key=lambda kv: kv[1])\n",
    "        is_confident = best_score >= threshold\n",
    "        out = {\n",
    "            \"predicted_category\": best_cat if is_confident else \"none\",\n",
    "            \"category_score\": best_score,\n",
    "            \"top_keyword\": cat_best_label[best_cat][i],\n",
    "            \"all_category_scores\": per_cat,\n",
    "            \"explanation_top3_keywords\": cat_label_scores[best_cat][i][:3] if cat_label_scores[best_cat][i] is not None else [],\n",
    "            \"sentiment\": sent_results[i],\n",
    "        }\n",
    "        outputs.append(out)\n",
    "    return outputs\n",
    "\n",
    "def batch_compute_topic_signed_scores(texts, topk_mean=3, threshold=0.25, batch_size=32):\n",
    "    \"\"\"\n",
    "    Returns (topic_scores_raw, details)\n",
    "    - topic_scores_raw: dict[topic] -> mean signed score in [-1, 1]\n",
    "    - details: list of per-post dicts (like your previous details)\n",
    "    Implemented using batch_classify_texts to avoid sequential model calls.\n",
    "    \"\"\"\n",
    "    outs = batch_classify_texts(texts, topk_mean=topk_mean, threshold=threshold, batch_size=batch_size)\n",
    "    per_topic_values = defaultdict(list)\n",
    "    details = []\n",
    "    # helper sign mapping compatible with your CHS code\n",
    "    def _sentiment_to_sign(sent_label: str) -> int:\n",
    "        s = (sent_label or \"\").strip().lower()\n",
    "        if s.startswith(\"pos\"): return +1\n",
    "        if s.startswith(\"neu\"): return 0\n",
    "        if s.startswith(\"neg\"): return -1\n",
    "        return 0\n",
    "\n",
    "    for text, out in zip(texts, outs):\n",
    "        pred_cat = out.get(\"predicted_category\", \"none\")\n",
    "        pred_score = float(out.get(\"category_score\", 0.0))\n",
    "        sent_label = out.get(\"sentiment\", {}).get(\"label\", \"Neutral\")\n",
    "        sign = _sentiment_to_sign(sent_label)\n",
    "        if pred_cat in OECD_TOPICS and pred_score > 0:\n",
    "            signed = sign * pred_score\n",
    "            per_topic_values[pred_cat].append(signed)\n",
    "        details.append({\n",
    "            \"text\": text,\n",
    "            \"predicted_category\": pred_cat,\n",
    "            \"category_score\": pred_score,\n",
    "            \"sentiment\": sent_label,\n",
    "            \"signed_score\": sign * pred_score if pred_cat in OECD_TOPICS else 0.0,\n",
    "        })\n",
    "    topic_scores_raw = {topic: (sum(vals)/len(vals)) if vals else 0.0 for topic, vals in per_topic_values.items()}\n",
    "    for topic in OECD_TOPICS:\n",
    "        topic_scores_raw.setdefault(topic, 0.0)\n",
    "    return topic_scores_raw, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da83fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cities (batch):   0%|          | 0/186 [00:00<?, ?it/s]/tmp/ipykernel_3398638/2808195562.py:21: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(int(ts)).isoformat() + 'Z'\n",
      "Cities (batch): 100%|██████████| 186/186 [07:50<00:00,  2.53s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'api_1_analysis_batched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUT_FILE.with_name(\u001b[33m'\u001b[39m\u001b[33mapi_1_analysis_batched.json\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     65\u001b[39m     json.dump(results_batch, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mWrote batched analysis to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_FILE.with_name(\u001b[43mapi_1_analysis_batched\u001b[49m.json)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# brief summary\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results_batch[:\u001b[32m5\u001b[39m]:\n",
      "\u001b[31mNameError\u001b[39m: name 'api_1_analysis_batched' is not defined"
     ]
    }
   ],
   "source": [
    "# 4) Run the batched analysis per city (fast)\n",
    "results_batch = []\n",
    "\n",
    "for city in tqdm(data, desc='Cities (batch)'):\n",
    "    city_name = city.get('city')\n",
    "    posts = city.get('posts', []) or []\n",
    "    texts_all = [p.get('text', '') for p in posts if p.get('text')]\n",
    "\n",
    "    if texts_all:\n",
    "        # overall (uses batched classification under the hood)\n",
    "        raw_overall, details_overall = batch_compute_topic_signed_scores(texts_all, topk_mean=3, threshold=0.25, batch_size=32)\n",
    "        topic_0_10 = normalize_topic_scores_0_10(raw_overall)\n",
    "        chs = compute_CHS(topic_0_10)\n",
    "\n",
    "        # per-post outputs: classify all texts in a batch and derive per-post topic vectors\n",
    "        outs = batch_classify_texts(texts_all, topk_mean=3, threshold=0.25, batch_size=32)\n",
    "        timeseries = []\n",
    "        # keep chronological order matching posts_sorted\n",
    "        posts_sorted = sorted(posts, key=lambda p: p.get('posted_at_timestamp', 0))\n",
    "        for p, out in zip(posts_sorted, outs):\n",
    "            ts = p.get('posted_at_timestamp')\n",
    "            text = p.get('text', '')\n",
    "            pred_cat = out.get('predicted_category', 'none')\n",
    "            pred_score = float(out.get('category_score', 0.0))\n",
    "            sent_label = out.get('sentiment', {}).get('label', 'Neutral')\n",
    "            # map sentiment to sign\n",
    "            s = (sent_label or '').strip().lower()\n",
    "            if s.startswith('pos'): sign = +1\n",
    "            elif s.startswith('neu'): sign = 0\n",
    "            elif s.startswith('neg'): sign = -1\n",
    "            else: sign = 0\n",
    "            signed = sign * pred_score\n",
    "            # per-post raw topic vector (only predicted category has non-zero signed contribution)\n",
    "            per_post_raw = {t: 0.0 for t in OECD_TOPICS}\n",
    "            if pred_cat in OECD_TOPICS:\n",
    "                per_post_raw[pred_cat] = signed\n",
    "            per_post_0_10 = normalize_topic_scores_0_10(per_post_raw)\n",
    "            chs_post = compute_CHS(per_post_0_10)\n",
    "\n",
    "            timeseries.append({\n",
    "                'posted_at_timestamp': ts,\n",
    "                'iso': ts_to_iso(ts) if ts is not None else '',\n",
    "                'topic_scores_0_10': per_post_0_10,\n",
    "                'chs': chs_post,\n",
    "                'text': text,\n",
    "            })\n",
    "\n",
    "    else:\n",
    "        topic_0_10 = {t: 0.0 for t in OECD_TOPICS}\n",
    "        chs = 0.0\n",
    "        timeseries = []\n",
    "\n",
    "    results_batch.append({\n",
    "        'city': city_name,\n",
    "        'lat': city.get('lat'),\n",
    "        'lng': city.get('lng'),\n",
    "        'n_posts': len(posts),\n",
    "        'overall_topic_scores_0_10': topic_0_10,\n",
    "        'overall_chs': chs,\n",
    "        'timeseries': timeseries,\n",
    "    })\n",
    "\n",
    "# save batched results\n",
    "with open(OUT_FILE.with_name('api_1_analysis_batched.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_batch, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845ddba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading n_posts= 23 overall_chs= 5.0\n",
      "Worth n_posts= 14 overall_chs= 4.6\n",
      "Liverpool n_posts= 13 overall_chs= 4.8\n",
      "Deal n_posts= 10 overall_chs= 4.6\n",
      "Boston n_posts= 8 overall_chs= 5.0\n"
     ]
    }
   ],
   "source": [
    "# print(f'Wrote batched analysis to: {OUT_FILE.with_name(api_1_analysis_batched.json)}')\n",
    "\n",
    "# brief summary\n",
    "for r in results_batch[:5]:\n",
    "    print(r['city'], 'n_posts=', r['n_posts'], 'overall_chs=', round(r['overall_chs'],1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HTB2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
